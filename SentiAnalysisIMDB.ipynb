{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {val:key for (key, val) in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "rev = ' '.join(index_to_word.get(val-3 , 'UNK') for val in X_train[1])\n",
    "print(rev)\n",
    "\n",
    "print(Y_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "Y = np.concatenate((Y_train, Y_test), axis=0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = np.unique(np.hstack(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88585\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 32)           2834784   \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                1024064   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,863,073\n",
      "Trainable params: 3,863,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(88587, 32, input_length=500))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 220s 9ms/step - loss: 0.3650 - acc: 0.8270 - val_loss: 0.2833 - val_acc: 0.8849\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 216s 9ms/step - loss: 0.1430 - acc: 0.9489 - val_loss: 0.3033 - val_acc: 0.8789\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 218s 9ms/step - loss: 0.0674 - acc: 0.9783 - val_loss: 0.4112 - val_acc: 0.8710\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 218s 9ms/step - loss: 0.0335 - acc: 0.9899 - val_loss: 0.5420 - val_acc: 0.8498\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 219s 9ms/step - loss: 0.0275 - acc: 0.9910 - val_loss: 0.5875 - val_acc: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5c5bad1d0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.95%\n",
      "[0.5875436723542213, 0.85948]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1]\n",
      "[1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[21:58])\n",
    "print(np.hstack(result[21:58]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\"\n",
    "text = \"bad management\"\n",
    "\n",
    "text_split = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "text = \"it was average value but didn't up to the mark\"\n",
    "text_split = text.lower().split()\n",
    "\n",
    "text_to_index = [word_index[val]+3 if val in word_index else 0 for val in text_split ]\n",
    "text_to_index = sequence.pad_sequences([text_to_index], 500)\n",
    "res = model.predict_classes([text_to_index])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 50)           4429350   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 496, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 124, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 71        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,483,285\n",
      "Trainable params: 4,483,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout, Conv1D, MaxPooling1D, LSTM, Activation\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(88587, \n",
    "                    50, \n",
    "                    input_length=500))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(64,\n",
    "                5,\n",
    "                padding='valid',\n",
    "                activation='relu',\n",
    "                strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(70))\n",
    "\n",
    "# Squash\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
